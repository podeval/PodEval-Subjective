# Subjective Listening Tests for Podcast Evaluation

A comprehensive web-based framework for conducting subjective audio quality assessment of podcast content, built on [webMUSHRA](https://github.com/audiolabs/webMUSHRA) with custom modifications for podcast evaluation requirements.


## ðŸŽ¯ Overview

This framework provides two specialized subjective evaluation methods for podcast audio assessment:

1. **Dialogue Naturalness Evaluation** - Focused assessment of conversational quality
2. **Questionnaire-based MOS Test** - Comprehensive multi-dimensional evaluation



### Dialogue Naturalness Evaluation

- **Purpose**: Evaluate the naturalness and authenticity of dialogue speech in podcast.
- **Features**:
   - Incorporates both high-quality (Real-Pod) and low-quality anchors ([eSpeak](https://espeak.sourceforge.net/)) to establish reliable quality references, which can also help identify inattentive evaluators and filter invalid submissions
   - Utilize dialogue segments featuring turn-taking between speakers (see `Dialogue Extraction` in [Real_Pod/](./Real_Pod/))
   - Samples from different systems presented on the same page with Real-Pod reference
   - Scoring is adjusted using a slider ranging from 0 to 100, divided into five stages with a clear definition.
- **Template**: `configs/dialogue.yaml`
- **Demo**:
   - **Website**: [Dialogue Naturalness Evaluation](http://18.139.172.240:8001/?config=dialogue_spon.yaml)
   - *Note: Server may be unavailable or unstable due to expiration or high traffic*

### Questionnaire-based MOS Test

- **Purpose**: Comprehensive evaluation of long-form podcast content through structured questionnaires.
- **Features**:
   - **MOS Test Format**: Evaluators listen to one audio sample at a time
   - **Long-form Content Handling**: Preprocesses audio by extracting first/middle/final minutes, and concatenates segments with beep separators (see `Audio Summarization` in [Real_Pod/](./Real_Pod/))
   - **Multi-dimensional Assessment**: 7 questions covering various quality dimensions
     - Audio Quality, Information Delivery Effectiveness, Speaker Expression Preference, Music/Sound Harmony, Engagement Level, Full Episode Willingness, Human Likelihood.
   - **Data Validity Enhancement**:
      - Attention-check Questions: 
         - "How many speakers are there in the podcast?"
         - "If music or sound effects... (Select Neutral if none are present)"
      - Justification Requirements: Users must provide explanations for responses
- **Template**: `configs/questionnaire.yaml`
- **Demo**: 
   - **Website**: [Questionnaire-based MOS Test](http://18.139.172.240:8001/?config=questionnaire_survey1_2_3.yaml)
   - *Note: Server may be unavailable or unstable due to expiration or high traffic*




## ðŸš€ Setup and Installation


### Quick Start with PHP Built-in Server

1. **Start the server**:
   ```bash
   # For local testing
   php -S localhost:8000
   
   # For public access
   php -S 0.0.0.0:8000
   ```

2. **Upload test audio files** to the appropriate directory

3. **Access the evaluation interface**:
   - **Dialogue Naturalness**: `http://localhost:8000/?config=dialogue.yaml`
   - **Questionnaire MOS**: `http://localhost:8000/?config=questionnaire.yaml`

### webMUSHRA + AWS EC2

- [A guideline for your reference]()




## ðŸ“š Acknowledgments

- [webMUSHRA](https://github.com/audiolabs/webMUSHRA) - Original framework

## ðŸ“„ License

This project is based on webMUSHRA and follows the same licensing terms. See the main project directory for complete license information.


---

*This framework is part of the PodEval toolkit for comprehensive podcast evaluation. For more information, see the main [PodEval README](../README.md).*